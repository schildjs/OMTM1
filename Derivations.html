<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.0.36">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Chiara Di Gravio, Jonathan Schildcrout">
<meta name="dcterms.date" content="2022-09-20">

<title>Marginalized, partial proportional odds models: Mathematical derivations for the likelihood and score</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>


<script src="Derivations_files/libs/clipboard/clipboard.min.js"></script>
<script src="Derivations_files/libs/quarto-html/quarto.js"></script>
<script src="Derivations_files/libs/quarto-html/popper.min.js"></script>
<script src="Derivations_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="Derivations_files/libs/quarto-html/anchor.min.js"></script>
<link href="Derivations_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="Derivations_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="Derivations_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="Derivations_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="Derivations_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body>

<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Marginalized, partial proportional odds models: Mathematical derivations for the likelihood and score</h1>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Chiara Di Gravio, Jonathan Schildcrout </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">September 20, 2022</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#notation" id="toc-notation" class="nav-link active" data-scroll-target="#notation">Notation</a></li>
  <li><a href="#the-marginalized-model" id="toc-the-marginalized-model" class="nav-link" data-scroll-target="#the-marginalized-model">The Marginalized Model</a>
  <ul class="collapse">
  <li><a href="#calculate-delta_itk" id="toc-calculate-delta_itk" class="nav-link" data-scroll-target="#calculate-delta_itk">Calculate <span class="math inline">\(\Delta_{it,k}\)</span></a></li>
  </ul></li>
  <li><a href="#notes-about-conditional-mean-model" id="toc-notes-about-conditional-mean-model" class="nav-link" data-scroll-target="#notes-about-conditional-mean-model">Notes about conditional mean model</a></li>
  <li><a href="#the-likelihood" id="toc-the-likelihood" class="nav-link" data-scroll-target="#the-likelihood">The Likelihood</a></li>
  <li><a href="#gradients" id="toc-gradients" class="nav-link" data-scroll-target="#gradients">Gradients</a>
  <ul class="collapse">
  <li><a href="#calculation-of-fracpartial-boldsymboldelta_itpartial-boldsymboltheta" id="toc-calculation-of-fracpartial-boldsymboldelta_itpartial-boldsymboltheta" class="nav-link" data-scroll-target="#calculation-of-fracpartial-boldsymboldelta_itpartial-boldsymboltheta">Calculation of <span class="math inline">\(\frac{\partial \boldsymbol{\Delta}_{it}}{\partial \boldsymbol{\theta}}\)</span></a></li>
  </ul></li>
  </ul>
</nav>
</div>
<main class="content quarto-banner-title-block" id="quarto-document-content">




<div class="hidden">
<p>\newcommand{}{<em>{it,2}^m} \newcommand{1g}{</em>{it-1,g}^m}</p>
\newcommand{1k}{_{it-1,k}^m}
<p>\newcommand{}{<em>{it,1}} \newcommand{}{</em>{it,2}}</p>
<p>\newcommand{1g}{<em>{it-1,g}} \newcommand{1g}{</em>{it-1,g}} \newcommand{1K1}{_{it-1,K-1}}</p>
<p>\newcommand{1g}{^{1g}} \newcommand{}{^{(K-1)(K-1)}} \newcommand{}{^{11}}</p>
<p>\newcommand{1ga}{^{1g(a)}} \newcommand{1a}{^{(K-1)(K-1)(a)}} \newcommand{11a}{^{11(a)}} \newcommand{1g1}{^{1g(1)}} \newcommand{}{^{(K-1)g(1)}} \newcommand{1gA}{^{1g(A)}} </p>
</div>
<div class="cell">

</div>
<section id="notation" class="level1">
<h1>Notation</h1>
<p><span class="math inline">\(Y_{it}\)</span>: ordinal response for subject <span class="math inline">\(i \in \{1, \dots N\}\)</span> on day <span class="math inline">\(t \in \{1, \dots T_i\}\)</span> with values <span class="math inline">\(k \in \{1, \dots, K \}\)</span></p>
<p><span class="math inline">\(Y_{it,k}=I(Y_{it}=k)\)</span></p>
<p><span class="math inline">\(Z_{it,k}=\sum_{l=1}^k Y_{it,k}= I(Y_{it}\leq k)\)</span></p>
<p><span class="math inline">\(\mathbf{X}_i\)</span> is the design matrix for univariate / cross-sectional model and <span class="math inline">\(\mathbf{X}_{it}\)</span> is the design vector on day <span class="math inline">\(t\)</span>.</p>
<p><span class="math inline">\(\mathbf{U}_i\)</span> is the design matrix for the response dependence model and <span class="math inline">\(\mathbf{U}_{it}\)</span> is the design vector on day <span class="math inline">\(t\)</span>. In many cases, covariates will not modify the response dependence model and <span class="math inline">\(u_{it}\)</span> will just be a 1.</p>
<p>Noting that conditioning on <span class="math inline">\(\mathbf{X}_i\)</span> is implicit, then</p>
<ul>
<li><p><span class="math inline">\(\pi_{it,k}^m= pr(Y_{it}\leq k)\)</span> is the marginal cumulative probability that captures how <span class="math inline">\(Y\)</span> depends on <span class="math inline">\(X\)</span></p></li>
<li><p><span class="math inline">\(\mu_{it,k}^m= pr(Y_{it}=k) = \pi_{it,k}^m- \pi_{it,k-1}^m\)</span> is the marginal probability of being in state <span class="math inline">\(k\)</span>.</p></li>
<li><p><span class="math inline">\(\mu_{it,k}^c= pr(Y_{it}=k \mid Y_{it-1})\)</span> is the conditional (on the lagged response) probability of being in state <span class="math inline">\(k\)</span>. It captures how <span class="math inline">\(Y\)</span>s depend on each other. Here we are assuming a first-order Markov model.</p></li>
<li><p><span class="math inline">\(h_{it,kl}= pr(Y_{it}=k \mid Y_{it-1}=l)\)</span> where conditioning on <span class="math inline">\(u_{it}\)</span> is implicit</p></li>
</ul>
</section>
<section id="the-marginalized-model" class="level1">
<h1>The Marginalized Model</h1>
<p>The marginalized transition model for ordinal response data is given by the following two models:</p>
<p><span class="math display">\[
\begin{aligned}
logit(\pi_{it,k}^m) &amp;= \eta_{it,k}= \alpha_k + \mathbf{X}_{it} \mathbf{\beta} \\
log \left( \frac{\mu_{it,k}^c}{\mu_{it,K}^c} \right) &amp;= \Delta_{it,k}(\mathbf{X}_i) + \sum_{l=1}^{K-1} u_{it}\gamma^{kl}Y_{it-1,l}
\end{aligned}
\]</span></p>
<p>where the former is a cumulative probability, proportional odds model, and the latter is a log relative risk model for nominal data. We do not use the ordering for the response depdendence model (yet). If we have <span class="math inline">\(\eta_{it,k}\)</span> and <span class="math inline">\(\gamma^{kl}\ \forall \ k,l\)</span> then we can solve for the implicit function <span class="math inline">\(\Delta_{it,k}\)</span> that constrains the marginal model and conditional model to be coherent. Once we have <span class="math inline">\(\Delta_{it,k}\)</span>, then we can capture <span class="math inline">\(\mu_{it,k}^c\)</span> for all <span class="math inline">\(k\)</span>, and as we will show, we can then identify the likelihood contribution for subject <span class="math inline">\(i\)</span> at time <span class="math inline">\(t\)</span>. The likelihood contribution is derived from <span class="math inline">\(\mu_{it,k}^c\)</span>. The <span class="math inline">\(\Delta_{it,k}\)</span> can be thought of as an offset or an intercept for the conditional mean model. Note <span class="math inline">\(\gamma^{kl}\)</span> may be a scalar or a vector depending on <span class="math inline">\(u_{it}\)</span>.</p>
<section id="calculate-delta_itk" class="level2">
<h2 class="anchored" data-anchor-id="calculate-delta_itk">Calculate <span class="math inline">\(\Delta_{it,k}\)</span></h2>
<p>Since the marginal mean model must equal the marginalized, conditional mean model (where we marginalize over the distribution of <span class="math inline">\(Y_{it-1}\)</span>), then</p>
<p><span class="math display">\[ \mu_{it,k}^m= \sum_{g=1}^{K} h_{it,kg}\cdot \mu_{it,1}^mg = \sum_{g=1}^{K} \frac{exp(\Delta_{it,k}+ u_{it}\gamma^{kg})}{1+\sum_{l=1}^{K-1}exp(\Delta_{it,l}+ u_{it}\gamma^{lg})} \cdot \mu_{it,1}^mg\]</span></p>
<p>We need to solve this equation for <span class="math inline">\(\Delta_{it,k}\ \forall \ i,t,k\)</span>, and to do this we implement a Newton Raphson algorithm. Let</p>
<p><span class="math display">\[f(\mathbf{\Delta}_{it})= \{f_k(\mathbf{\Delta}_{it}) \}_{k=1}^{K}=  \left\{\sum_{g=1}^{K} h_{it,kg}\cdot \mu_{it,1}^mg - \mu_{it,k}^m\right\}_{k=1}^{K}\]</span></p>
<p>be a vector of length <span class="math inline">\(K\)</span> and for all <span class="math inline">\(k=\{1, \dots K \}\)</span>, solve <span class="math inline">\(f_k(\mathbf{\Delta}_{it})=0\)</span>. If we look closely <span class="math inline">\(h_{it,kg}\)</span>, we see that it depends on all <span class="math inline">\(\{\Delta_{it,1}, \dots \Delta_{it,K-1}\}\)</span>, so the gradient of <span class="math inline">\(f(\mathbf{\Delta}_{it})\)</span> is a non-diagonal matrix. At iteration <span class="math inline">\(m\)</span> of the NR algorithm, we have</p>
<p><span class="math display">\[\mathbf{\Delta}_{it}^{(m)} = \mathbf{\Delta}_{it}^{(m-1)} - \left( \frac{ \partial f(\mathbf{\Delta}_{it}^{(m-1)})}{\partial \mathbf{\Delta}_{it}^{(m-1)}}   \right)^{-1} f(\mathbf{\Delta}_{it}^{(m-1)})\]</span></p>
<p>where</p>
<p><span class="math display">\[
\frac{ \partial f_j(\mathbf{\Delta}_{it})}{\partial \Delta_{it,k}} =
\begin{cases}
\sum_{g=1}^{K} h_{it,kg}\cdot (1- h_{it,kg}) \cdot \mu_{it,1}^mg,&amp; \ \mbox{if $j=k$} \\
\sum_{g=1}^{K} -h_{it,kg}\cdot h_{it,jg}\cdot \mu_{it,1}^mg,&amp; \ \mbox{if $j \neq k$} \\
\end{cases}
\]</span></p>
</section>
</section>
<section id="notes-about-conditional-mean-model" class="level1">
<h1>Notes about conditional mean model</h1>
<ul>
<li><span class="math inline">\(h_{it,kg}= pr(Y_{it,k}\mid Y_{it-1}=g) = \frac{exp(\Delta_{it,k}+ u_{it}\gamma^{kg})}{1+\sum_{l=1}^{K-1} exp(\Delta_{it,l}+ u_{it}\gamma^{lg})}\)</span></li>
<li><span class="math inline">\(h_{it,kK}= pr(Y_{it,k}\mid Y_{it-1}=K)= \frac{exp(\Delta_{it,k})}{1+\sum_{l=1}^{K-1} exp(\Delta_{it,l})}\)</span> because state <span class="math inline">\(K\)</span> is the reference state for lagged values (i.e.&nbsp;<span class="math inline">\(\gamma^{kK}=0\)</span>). So <span class="math inline">\(\gamma\)</span> does not appear in the <span class="math inline">\(K^{th}\)</span> column of the <span class="math inline">\(\mathbf{h}_{it}\)</span> matrix.</li>
<li><span class="math inline">\(h_{it,Kg}= 1-\sum_{k=1}^{K-1} pr(Y_{it,k}\mid Y_{it-1}=g) = 1-\sum_{k=1}^{K-1} h_{it,kg}= \frac{1}{1+\sum_{l=1}^{K-1} exp(\Delta_{it,l}+ u_{it}\gamma^{lg})}\)</span> because state <span class="math inline">\(K\)</span> is the reference state for the outcome</li>
<li>In the <span class="math inline">\(\mathbf{h}_{it}\)</span> matrix (<span class="math inline">\(K \times K\)</span>), rows correspond to <span class="math inline">\(Y_{it}\)</span> and columns correspond to <span class="math inline">\(Y_{it-1}\)</span>, so that the <span class="math inline">\((k,g)^{th}\)</span> element is <span class="math inline">\(h_{it,kg}= pr(Y_{it,k}\mid Y_{it-1}=g)\)</span>.</li>
<li><span class="math inline">\(\Delta_{it,k}\)</span> is the fixed intercept for a row in <span class="math inline">\(\mathbf{h}_{it}\)</span></li>
</ul>
<p><span class="math display">\[
\mathbf{h}_{it} = \left\{
        \begin{array}{cccc}
        h_{it,1 1} &amp; h_{it,1 2} &amp; \dots &amp; h_{it,1 K} \\
        h_{it,2 1} &amp; h_{it,2 2} &amp; \dots &amp; h_{it,2 K} \\
        \dots      &amp; \dots      &amp; \dots &amp; \dots \\
        h_{it,K-1 1} &amp; h_{it,K-1 2} &amp; \dots &amp; h_{it,K-1 K} \\
        1-\sum_{l=1}^{K-1} h_{it,l 1} &amp; 1-\sum_{l=1}^{K-1} h_{it,l 1} &amp; \dots &amp; 1-\sum_{l=1}^{K-1} h_{it,l 1} \\
        \end{array}
        \right\}
\]</span></p>
<ul>
<li><span class="math inline">\(log \left( \frac{ pr(Y_{it}=k \mid Y_{it-1}=g)}{pr(Y_{it}=K \mid Y_{it-1}=g)} \right) = \Delta_{it,k}+ u_{it}\gamma^{kg}\)</span> and <span class="math inline">\(log \left( \frac{ pr(Y_{it}=k \mid Y_{it-1}=K)}{pr(Y_{it}=K \mid Y_{it-1}=K)} \right) = \Delta_{it,k}\)</span>, so</li>
</ul>
<p><span class="math display">\[\gamma^{kg}=  log \left( \frac{ pr(Y_{it}=k \mid Y_{it-1}=g)/pr(Y_{it}=K \mid Y_{it-1}=g)  }{ pr(Y_{it}=k \mid Y_{it-1}=K)/pr(Y_{it}=K \mid Y_{it-1}=K) } \right)  = log \left( \frac{ h_{it,k g} / h_{it,K g}  }{  h_{it,k K} / h_{it,K K} } \right)\]</span></p>
<p>is not that easy (for me) to interpret by itself bc we need to know values of all elements of <span class="math inline">\(\mathbf{\Delta}_{it}\)</span>.</p>
<ul>
<li>Think of <span class="math inline">\(u_{it}\{ \gamma^{kg}\}\)</span> as a <span class="math inline">\((K-1) \times (K-1)\)</span> matrix of log relative risk ratios (that depend on <span class="math inline">\(u_{it}\)</span>) for transitions from state <span class="math inline">\(g\)</span> to state <span class="math inline">\(k\)</span>, where rows <span class="math inline">\(k\)</span> correspond to outcomes on day <span class="math inline">\(t\)</span> and columns <span class="math inline">\(g\)</span> correspond to outcomes on day <span class="math inline">\(t-1\)</span>.</li>
</ul>
</section>
<section id="the-likelihood" class="level1">
<h1>The Likelihood</h1>
<p>Since this is a first-order transition model, we split the likelihood contribution for each subject into the 1) the first observation, and 2) all other observations. Let <span class="math inline">\(\mathbf{\theta}=(\mathbf{\alpha}, \mathbf{\beta}, \mathbf{\gamma})\)</span></p>
<ul>
<li><span class="math inline">\(L(\mathbf{\theta}) = \prod_{i=1}^N L_i(\mathbf{\theta})\)</span></li>
<li><span class="math inline">\(l(\mathbf{\theta}) = \sum_{i=1}^N l_i(\mathbf{\theta}) = \sum_{i=1}^N log L_i(\mathbf{\theta})\)</span></li>
</ul>
<p>The log-likelihood contribution by subject <span class="math inline">\(i\)</span> can be written,</p>
<p><span class="math display">\[
\begin{aligned}
l_i(\mathbf{\theta}) =&amp;  l_i^{(1)}(\mathbf{\beta},\mathbf{\alpha}) + l_i^{(2)}(\mathbf{\theta}) \\
=&amp; \sum_{k=1}^{K} Y_{i1,k}log (\mu_{i1,k}^m) + \sum_{t=2}^{T_i} \sum_{k=1}^{K} Y_{it,k}log (\mu_{it,k}^c)
\end{aligned}
\]</span></p>
<p>Relying on Lee and Daniels (2007) and McCullagh (1980), and for computational efficiency, we can write <span class="math inline">\(l_i^{(1)}(\mathbf{\beta},\mathbf{\alpha})\)</span> as follows</p>
<p><span class="math display">\[
\begin{aligned}
l_i^{(1)}(\mathbf{\beta},\mathbf{\alpha})= \sum_{k=1}^{K-1} [Z_{i1,k}\phi_{i1,k}- Z_{i1,k+1}g(\phi_{i1,k})]
\end{aligned}
\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(\phi_{it,k}= log \left( \frac{\pi_{it,k}^m}{\pi_{it,k+1}^m-\pi_{it,k}^m} \right)\)</span></li>
<li><span class="math inline">\(g(\phi_{it,k}) = log(1+exp(\phi_{it,k})) = log \left( \frac{\pi_{it,k+1}^m}{\pi_{it,k+1}^m-\pi_{it,k}^m} \right)\)</span></li>
</ul>
<p>We can write <span class="math inline">\(l_i^{(2)}(\mathbf{\theta})\)</span> in exponential family form as follows</p>
<p><span class="math display">\[
\begin{aligned}
l_i^{(2)}(\mathbf{\theta})= \sum_{t=2}^{T_i} \left[ \sum_{k=1}^{K-1} Y_{it,k}(\Delta_{it,k}+ \sum_{l=1}^{K-1} u_{it}\gamma^{kl}Y_{it-1,l}) - log(1+\sum_{k=1}^{K-1} exp(\Delta_{it,k}+ \sum_{l=1}^{K-1} u_{it}\gamma^{kl}Y_{it-1,l})) \right]
\end{aligned}
\]</span></p>
</section>
<section id="gradients" class="level1">
<h1>Gradients</h1>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial l_i^{(1)}(\mathbf{\beta},\mathbf{\alpha})}{\partial \theta_1}= \sum_{k=1}^{K-1} \left[ Z_{i1,k}\frac{\partial \phi_{i1,k}}{\partial \theta_1} - Z_{i1,k+1}\frac{\partial g(\phi_{i1,k})}{\partial \theta_1} \right]  
\end{aligned}
\]</span></p>
<p>where we can show that (see handwritten notes)</p>
<ul>
<li><span class="math inline">\(\frac{\partial \phi_{i1,k}}{\partial \theta_1}=exp(g(\phi_{i1,k})) \cdot \left[(1-\pi_{i1,k}^m) \frac{\partial \eta_{i1,k}}{\partial \theta_1} - (1-\pi_{i1,k+1}^m) \frac{\partial \eta_{i1,k+1}}{\partial \theta_1} \right]\)</span></li>
<li><span class="math inline">\(\frac{\partial g(\phi_{i1,k})}{\partial \theta_1}=exp(\phi_{i1,k}) \cdot \left[(1-\pi_{i1,k}^m) \frac{\partial \eta_{i1,k}}{\partial \theta_1} - (1-\pi_{i1,k+1}^m) \frac{\partial \eta_{i1,k+1}}{\partial \theta_1} \right] = \frac{exp(\phi_{i1,k})}{exp(g(\phi_{i1,k}))} \cdot \frac{\partial \phi_{i1,k}}{\partial \theta_1} = \frac{\pi_{i1,k}^m}{\pi_{i1,k+1}^m} \cdot \frac{\partial \phi_{i1,k}}{\partial \theta_1}\)</span></li>
</ul>
<p>so that</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial l_i^{(1)}(\mathbf{\beta},\mathbf{\alpha})}{\partial \theta_1}= \sum_{k=1}^{K-1} \left[ Z_{i1,k}- Z_{i1,k+1}\frac{\pi_{i1,k}^m}{\pi_{i1,k+1}^m} \right]  \frac{\partial \phi_{i1,k}}{\partial \theta_1}
\end{aligned}
\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(\frac{\pi_{i1,k}^m}{\pi_{i1,k+1}^m}=\frac{exp(\phi_{i1,k})}{1+exp(\phi_{i1,k})}\)</span></li>
<li><span class="math inline">\(\frac{\partial \phi_{i1,k}}{\partial \theta_1} = \frac{\partial \phi_{i1,k}}{\partial \pi_{i1,k}^m} \cdot \frac{\partial \pi_{i1,k}^m}{\partial \eta_{i1,k}} \cdot \frac{\partial \eta_{i1,k}}{\partial \theta_1} + \frac{\partial \phi_{i1,k}}{\partial \pi_{i1,k+1}^m} \cdot \frac{\partial \pi_{i1,k+1}^m}{\partial \eta_{i1,k+1}} \cdot \frac{\partial \eta_{i1,k+1}}{\partial \theta_1} = \frac{\partial \phi_{i1,k}}{\partial \eta_{i1,k}} \cdot \frac{\partial \eta_{i1,k}}{\partial \theta_1} + \frac{\partial \phi_{i1,k}}{\partial \eta_{i1,k+1}} \cdot \frac{\partial \eta_{i1,k+1}}{\partial \theta_1}\)</span>. This is from LD2007 and I checked that this is equal to what I got.</li>
</ul>
<!-- $$ -->
<!-- \begin{aligned} -->
<!-- # \frac{\partial l_i^{(2)}(\mathbf{\theta})}{\partial \theta_1}= & \sum_{t=2}^{T_i} \left[ \sum_{k=1}^{K-1} \Yitk \left(\frac{\partial \Deltaitk}{\partial \theta_1} + \sum_{l=1}^{K-1} \frac{\partial \gammakl}{\partial \theta_1} \yitONEl \right)  - \frac{exp(\Deltaitk + \sum_{l=1}^{K-1} \gammakl \yitONEl)}{(1+\sum_{k=1}^{K-1} exp(\Deltaitk + \sum_{l=1}^{K-1} \gammakl \yitONEl))} \left(\frac{\partial \Deltaitk}{\partial \theta_1} + \sum_{l=1}^{K-1} \frac{\partial \gammakl}{\partial \theta_1} \yitONEl \right) \right] \\  -->
<!-- \end{aligned} -->
<!-- $$ -->
<p>Next,</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial l_i^{(2)}(\mathbf{\theta})}{\partial \theta_1} =&amp; \sum_{t=2}^{T_i} \left[ \sum_{k=1}^{K-1} (Y_{it,k}- \mu_{it,k}^c) \left(\frac{\partial \Delta_{it,k}}{\partial \theta_1} + \sum_{l=1}^{K-1} u_{it}\frac{\partial \gamma^{kl}}{\partial \theta_1} Y_{it-1,l}\right)  \right]
\end{aligned}
\]</span></p>
<p>so that</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial l_i^{(2)}(\mathbf{\theta})}{\partial \alpha_k}= &amp; \sum_{t=2}^{T_i} \left[ \sum_{k=1}^{K-1} (Y_{it,k}- \mu_{it,k}^c) \left(\frac{\partial \Delta_{it,k}}{\partial \alpha_k}  \right)  \right] \\
\frac{\partial l_i^{(2)}(\mathbf{\theta})}{\partial \beta_b}= &amp; \sum_{t=2}^{T_i} \left[ \sum_{k=1}^{K-1} (Y_{it,k}- \mu_{it,k}^c) \left(\frac{\partial \Delta_{it,k}}{\partial \beta_j}  \right)  \right] \\
\frac{\partial l_i^{(2)}(\mathbf{\theta})}{\partial \gamma^{db(a)}}= &amp; \sum_{t=2}^{T_i} \left[ \sum_{k=1}^{K-1} (Y_{it,k}- \mu_{it,k}^c) \left(\frac{\partial \Delta_{it,k}}{\partial \gamma^{db(a)}} \right) \right] + \sum_{t=2}^{T_i}  (Y_{it,d}-\mu_{it,d}^c) u_{it}^{(a)}Y_{it,b}
\end{aligned}
\]</span></p>
<p>because <span class="math inline">\(\frac{\partial \gamma^{kl}}{\partial \gamma^{db(a)}}=1\)</span> if <span class="math inline">\(kl=db(a)\)</span> and is <span class="math inline">\(0\)</span> otherwise. <span class="math inline">\(u_{it}\)</span> is potentially a row vector of length <span class="math inline">\(A\)</span>, <span class="math inline">\(u_{it}^{(a)}\)</span> is the <span class="math inline">\(a^{th}\)</span> element and <span class="math inline">\(\gamma^{db(a)}\)</span> is the <span class="math inline">\(a^{th}\)</span> element of <span class="math inline">\(\gamma^{db}\)</span>, so that</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial}{\partial \gamma^{db(a)}} \left[ u_{it}\gamma^{kl}\right] = &amp;
\begin{cases}
u_{it}^{(a)}&amp; kl=db \\
0 &amp; \mbox{otherwise}
\end{cases}
\end{aligned}
\]</span></p>
<section id="calculation-of-fracpartial-boldsymboldelta_itpartial-boldsymboltheta" class="level2">
<h2 class="anchored" data-anchor-id="calculation-of-fracpartial-boldsymboldelta_itpartial-boldsymboltheta">Calculation of <span class="math inline">\(\frac{\partial \boldsymbol{\Delta}_{it}}{\partial \boldsymbol{\theta}}\)</span></h2>
<p><span class="math inline">\(\frac{\partial \boldsymbol{\Delta}_{it}}{\partial \boldsymbol{\theta}}\)</span> is a <span class="math inline">\((K-1) \times npar\)</span> matrix, with rows denoted by <span class="math inline">\(k\)</span> and columns corresponding to individual parameters <span class="math inline">\(\theta_1 \in \boldsymbol{\theta}\)</span>. To calculate this matrix we have to solve a system of equations <span class="math inline">\(\boldsymbol{P}\boldsymbol{Q}=\boldsymbol{R}\)</span> where <span class="math inline">\(\boldsymbol{Q}=\frac{\partial \boldsymbol{\Delta}_{it}}{\partial \boldsymbol{\theta}}\)</span>.</p>
<p>Now, consider a single parameter, <span class="math inline">\(\theta_1\)</span> and outcome level, <span class="math inline">\(k\)</span> which involves a single element in the <span class="math inline">\(\frac{\partial \boldsymbol{\Delta}_{it}}{\partial \boldsymbol{\theta}}\)</span> matrix. We have</p>
<p><span class="math display">\[
\begin{aligned}
\mu_{it,k}^m= &amp; \sum_{g=1}^{K} h_{it,kg}\mu_{it,1}^mg  \\
\frac{\partial \mu_{it,k}^m}{\partial \theta_1} = &amp; \sum_{g=1}^{K} \left[ \frac{\partial h_{it,kg}}{\partial \theta_1}\mu_{it,1}^mg   + h_{it,kg}\frac{\partial \mu_{it,1}^mg}{\partial \theta_1} \right] \\
\frac{\partial h_{it,kg}}{\partial \theta_1}\mu_{it,1}^mg = &amp; \frac{\partial \mu_{it,k}^m}{\partial \theta_1} - h_{it,kg}\frac{\partial \mu_{it,1}^mg}{\partial \theta_1} \\
\end{aligned}
\]</span></p>
<p>Since <span class="math inline">\(h_{it,kg}=h(\mathbf{\Delta}_{it}1, \dots \Delta_{it,K-1}, \gamma1g1, \dots \gamma^{(K-1)g}1, \dots \gamma1gA, \dots \gamma^{(K-1)g(A)})\)</span></p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial h_{it,kg}}{\partial \theta_1} = &amp; \sum_{j=1}^{K-1} \frac{\partial h_{it,kg}}{\partial \Delta_{it,j}} \frac{\partial \Delta_{it,j}}{\partial \theta_1} + \sum_{a=1}^{A} \sum_{j=1}^{K=1}  \frac{\partial h_{it,kg}}{\partial \gamma^{jg(a)}} \frac{\partial \gamma^{jg(a)}}{\partial \theta_1}
\end{aligned}
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
\underbrace{\sum_{g=1}^{K}   \sum_{j=1}^{K-1}  \frac{\partial h_{it,kg}}{\partial \Delta_{it,j}} \frac{\partial \Delta_{it,j}}{\partial \theta_1}\mu_{it,1}^mg}_{(i)} = - \underbrace{ \sum_{g=1}^{K} \sum_{a=1}^{A} \sum_{j=1}^{K-1} \frac{\partial h_{it,kg}}{\partial \gamma^{jg}} \frac{\partial \gamma^{jg}}{\partial \theta_1} \mu_{it,1}^mg}_{(ii)}  + \underbrace{\frac{\partial \mu_{it,k}^m}{\partial \theta_1}}_{(iii)} - \sum_{g=1}^{K} \underbrace{h_{it,kg}\frac{\partial \mu_{it,1}^mg}{\partial \theta_1}}_{(iv)}
\end{aligned}
\]</span></p>
<p>We need to solve the above for all <span class="math inline">\(K-1 \times npar\)</span> outcome level by parameter combinations simultaneously. The value <span class="math inline">\((i)\)</span> corresponds to the multiplkation of <span class="math inline">\(k^{th}\)</span> row of the left matrix <span class="math inline">\(\boldsymbol{P}\)</span> with the column corresponding to <span class="math inline">\(\theta_1\)</span> in the right matrix <span class="math inline">\(\boldsymbol{Q}\)</span>:</p>
<p><span class="math display">\[
\underbrace{
\left[
\begin{array}{cccc}
\frac{\partial h_{it,1*}}{\partial \Delta_{it,1}} \cdot \mu_{it-1,*} &amp;  \frac{\partial h_{it,1*}}{\partial \Delta_{it,2}}\mu_{it-1,*} &amp; \dots &amp; \frac{\partial h_{it,1*}}{\partial \Delta_{it,K-1}}\mu_{it-1,*} \\
\frac{\partial h_{it,2*}}{\partial \Delta_{it,1}} \cdot \mu_{it-1,*} &amp;  \frac{\partial h_{it,2*}}{\partial \Delta_{it,2}}\mu_{it-1,*} &amp; \dots &amp; \frac{\partial h_{it,2*}}{\partial \Delta_{it,K-1}}\mu_{it-1,*} \\
. &amp;&amp; \dots &amp; \\
. &amp;&amp; \dots &amp; \\
. &amp;&amp; \dots &amp; \\
\frac{\partial h_{it,K-1*}}{\partial \Delta_{it,1}} \cdot \mu_{it-1,*} &amp;  \frac{\partial h_{it,K-1*}}{\partial \Delta_{it,2}}\mu_{it-1,*} &amp; \dots &amp; \frac{\partial h_{it,K-1*}}{\partial \Delta_{it,K-1}}\mu_{it-1,*}
\end{array}
\right]
}_{\boldsymbol{P} \sim (K-1) \times (K-1)}
\underbrace{\left[
\begin{array}{cccc}
\frac{\partial  \Delta_{it,1}}{\partial \theta_1 } &amp; \frac{\partial  \Delta_{it,1}}{\partial \theta_2 } \dots \frac{\partial  \Delta_{it,1}}{\partial \theta_{npar} } \\
\frac{\partial  \Delta_{it,2}}{\partial \theta_1 }  &amp; \frac{\partial  \Delta_{it,2}}{\partial \theta_2 } \dots \frac{\partial  \Delta_{it,2}}{\partial \theta_{npar} } \\
. \\
. \\
.\\
\frac{\partial  \Delta_{it,K-1}}{\partial \theta_1 }  &amp; \frac{\partial  \Delta_{it,K-1}}{\partial \theta_2 } \dots \frac{\partial  \Delta_{it,K-1}}{\partial \theta_{npar} }    
\end{array}
\right]}_{\boldsymbol{Q} \sim (K-1) \times npar}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\begin{aligned}
\frac{ \partial h_{it,kg}}{\partial \Delta_{it,j}} = &amp;
\begin{cases}
h_{it,kg}\cdot (1- h_{it,kg}),&amp; \ \mbox{if $j=k$} \\
-h_{it,kg}\cdot h_{it,jg},&amp; \ \mbox{if $j \neq k$}.
\end{cases}
\end{aligned}
\]</span></p>
<p>Assuming that <span class="math inline">\(K=4\)</span>, then <span class="math inline">\(\boldsymbol{P}\)</span> is equal to</p>
<p><span class="math display">\[
\left[
\begin{array}{ccc}
[\mathbf{h}_{it,1*} \circ (\mathbf{1}-\mathbf{h}_{it,1*})] \mathbf{\mu}_{it-1,*}^m &amp;
[\mathbf{h}_{it,1*} \circ (-\mathbf{h}_{it,2*})] \mathbf{\mu}_{it-1,*}^m &amp;
[\mathbf{h}_{it,1*} \circ (-\mathbf{h}_{it,3*})] \mathbf{\mu}_{it-1,*}^m \\
[\mathbf{h}_{it,2*} \circ (-\mathbf{h}_{it,1*})] \mathbf{\mu}_{it-1,*}^m &amp;
[\mathbf{h}_{it,2*} \circ (\mathbf{1}-\mathbf{h}_{it,2*})] \mathbf{\mu}_{it-1,*}^m &amp;
[\mathbf{h}_{it,2*} \circ (-\mathbf{h}_{it,3*})] \mathbf{\mu}_{it-1,*}^m \\
[\mathbf{h}_{it,3*} \circ (-\mathbf{h}_{it,1*})] \mathbf{\mu}_{it-1,*}^m &amp;
[\mathbf{h}_{it,3*} \circ (-\mathbf{h}_{it,2*})] \mathbf{\mu}_{it-1,*}^m &amp;
[\mathbf{h}_{it,3*} \circ (\mathbf{1}-\mathbf{h}_{it,3*})] \mathbf{\mu}_{it-1,*}^m \\
\end{array}
\right]
\]</span></p>
<p>where we use <span class="math inline">\(\circ\)</span> to denote Hadamard (elementwise) multiplication, <span class="math inline">\(\boldsymbol{h}_{it,k*}\)</span> is a row vector, and <span class="math inline">\(\boldsymbol{\mu}_{it-1,*}^m\)</span> is a column vector.</p>
<p>For <span class="math inline">\(\theta_1 \in (\mathbf{\alpha}, \mathbf{\beta})\)</span>, where <span class="math inline">\(ii=0\)</span>, a column of <span class="math inline">\(\boldsymbol{R}\)</span> corresponding to <span class="math inline">\(\theta_1\)</span> is equal to:</p>
<p><span class="math display">\[
\begin{aligned}
\left[
\begin{array}{c}
\frac{\partial \mu_{it,1}^m}{\partial \theta_1} - \sum_{g=1}^{K} h_{it,1 g} \frac{\partial \mu_{it,1}^mg}{\partial \theta_1} &amp; \\
\frac{\partial \mu_{it,2}^m}{\partial \theta_1} - \sum_{g=1}^{K} h_{it,2 g} \frac{\partial \mu_{it,1}^mg}{\partial \theta_1} &amp; \\
\dots \\
\frac{\partial \mu_{it,K-1}^m}{\partial \theta_1} - \sum_{g=1}^{K} h_{it,K-1 g} \frac{\partial \mu_{it,1}^mg}{\partial \theta_1}
\end{array}
\right]
=&amp;
\left[
\begin{array}{c}
\frac{\partial \mu_{it,1}^m}{\partial \theta_1} - h_{it,1 *} \frac{\partial \mu_{it-1,*}^m}{\partial \theta_1} &amp; \\
\frac{\partial \mu_{it,2}^m}{\partial \theta_1} -  h_{it,2 *} \frac{\partial \mu_{it-1,*}^m}{\partial \theta_1} &amp; \\
\dots \\
\frac{\partial \mu_{it,K-1}^m}{\partial \theta_1} - h_{it,K-1 *} \frac{\partial \mu_{it-1,*}^m}{\partial \theta_1}
\end{array}
\right]
\end{aligned}
\]</span></p>
<p>where, again <span class="math inline">\(\boldsymbol{h}_{it,k*}\)</span> is a row vector, and <span class="math inline">\(\frac{\partial \mu_{it-1,*}^m}{\partial \theta_1}\)</span> is a column vector. Across all of the <span class="math inline">\((\boldsymbol{\alpha},\boldsymbol{\beta})\)</span>, and assuming <span class="math inline">\(K=4\)</span> and <span class="math inline">\(\boldsymbol{x}_{it}= (x_{it,1}, x_{it,2})\)</span>, the <span class="math inline">\(\frac{\partial \boldsymbol{\mu}_{it}^m}{\partial \boldsymbol{\theta}}\)</span> is equal to</p>
<p><span class="math display">\[
\left[
\begin{array}{ccccc}
\frac{\partial \pi_{it,1}}{\partial \alpha_1} - 0 &amp; \frac{\partial \pi_{it,1}}{\partial \alpha_2} - 0 &amp; \frac{\partial \pi_{it,1}}{\partial \alpha_3} - 0 &amp; \frac{\partial \pi_{it,1}}{\partial \beta_1} - 0 &amp; \frac{\partial \pi_{it,1}}{\partial \beta_2} - 0 \\   
\frac{\partial \pi_{it,2}}{\partial \alpha_1} - \frac{\partial \pi_{it,1}}{\partial \alpha_1} &amp;
\frac{\partial \pi_{it,2}}{\partial \alpha_2} - \frac{\partial \pi_{it,1}}{\partial \alpha_2} &amp;
\frac{\partial \pi_{it,2}}{\partial \alpha_3} - \frac{\partial \pi_{it,1}}{\partial \alpha_3}  &amp;
\frac{\partial \pi_{it,2}}{\partial \beta_1} -  \frac{\partial \pi_{it,1}}{\partial \beta_1} &amp;   
\frac{\partial \pi_{it,2}}{\partial \beta_2} -  \frac{\partial \pi_{it,1}}{\partial \beta_2} \\
\frac{\partial \pi_{it,3}}{\partial \alpha_1} - \frac{\partial \pi_{it,2}}{\partial \alpha_1} &amp;
\frac{\partial \pi_{it,3}}{\partial \alpha_2} - \frac{\partial \pi_{it,2}}{\partial \alpha_2} &amp;
\frac{\partial \pi_{it,3}}{\partial \alpha_3} - \frac{\partial \pi_{it,2}}{\partial \alpha_3}  &amp;
\frac{\partial \pi_{it,3}}{\partial \beta_1} -  \frac{\partial \pi_{it,2}}{\partial \beta_1} &amp;   
\frac{\partial \pi_{it,3}}{\partial \beta_2} -  \frac{\partial \pi_{it,2}}{\partial \beta_2} \\
0 - \frac{\partial \pi_{it,3}}{\partial \alpha_1} &amp;
0 - \frac{\partial \pi_{it,3}}{\partial \alpha_2} &amp;
0 - \frac{\partial \pi_{it,3}}{\partial \alpha_3} &amp;
0 - \frac{\partial \pi_{it,3}}{\partial \beta_1}  &amp;   
0 - \frac{\partial \pi_{it,3}}{\partial \beta_2}
\end{array}
\right]
\]</span></p>
<p>which for programming purposes we get from</p>
<p><span class="math display">\[
\left[
\begin{array}{c}
\frac{\partial \pi_{it,1}}{\partial \eta_1} \\
\frac{\partial \pi_{it,2}}{\partial \eta_2} \\
\frac{\partial \pi_{it,3}}{\partial \eta_3} \\
0
\end{array}
\right]
\left[
\begin{array}{ccccc}
1 &amp; 0 &amp; 0 &amp; x_{it,1} &amp; x_{it,2} \\
0 &amp; 1 &amp; 0 &amp; x_{it,1} &amp; x_{it,2} \\
0 &amp; 0 &amp; 1 &amp; x_{it,1} &amp; x_{it,2} \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0
\end{array}
\right]
-
\left[
\begin{array}{c}
0 \\
\frac{\partial \pi_{it,1}}{\partial \eta_1} \\
\frac{\partial \pi_{it,2}}{\partial \eta_2} \\
\frac{\partial \pi_{it,3}}{\partial \eta_3} \\
\end{array}
\right]
\left[
\begin{array}{ccccc}
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
1 &amp; 0 &amp; 0 &amp; x_{it,1} &amp; x_{it,2} \\
0 &amp; 1 &amp; 0 &amp; x_{it,1} &amp; x_{it,2} \\
0 &amp; 0 &amp; 1 &amp; x_{it,1} &amp; x_{it,2} \\
\end{array}
\right]
\]</span></p>
<p>and the column vectors elements multiply the entire row of the matrix to the right. Note also that this is the precise form for <span class="math inline">\(\frac{\partial \boldsymbol{\mu}_{it-1}^m}{\partial \boldsymbol{\theta}}\)</span> which is pre-multiplied by the row vector <span class="math inline">\(\boldsymbol{h}_{it,k*}\)</span> <!-- For each element of $\theta_1$, there are $K-1$ unknown values $\left(\frac{\partial \Deltait1}{\partial \theta_1}, \dots \frac{\partial \DeltaitKONE}{\partial \theta_1}  \right)$ and there are $K-1$ equations.  So, we need to solve the system of equations.  With $npar$ parameters, we have $npar \dot (K-1)$ values of $\frac{\partial \Deltait}{\partial \bm{\theta}}$ for all $i$ and all $t$.  \\  --></p>
<!-- Across all parameters, we solve $\mathbf{P}\mathbf{Q}=\mathbf{R}$ for $\mathbf{Q}$ where $\mathbf{P} \sim (K-1)\times(K-1)$, $\mathbf{Q} \sim (K-1) \times npar$, and $\mathbf{R}\sim (K-1) \times npar = -ii+iii-iv$.  -->
<p><strong>For <span class="math inline">\(\theta_1 \in (\gamma^{11(1)}, \dots \gamma^{(K-1)(K-1)(A)})\)</span>, <span class="math inline">\((iii)=(iv)=0\)</span></strong> so that an element in <span class="math inline">\(\boldsymbol{R}\)</span> corresponding to a <span class="math inline">\(\gamma\)</span> parameter is equal to</p>
<p><span class="math display">\[
\begin{aligned}
- \sum_{g=1}^{K} \sum_{j=1}^{K=1} \sum_{a=1}^A \frac{\partial h_{it,kg}}{\partial \gamma^{jg(a)}} \frac{\partial \gamma^{jg(a)}}{\partial \theta_1} \mu_{it,1}^mg
\end{aligned}
\]</span></p>
<p>noting that since</p>
<p><span class="math display">\[
\begin{aligned}
\frac{ \partial \gamma^{jg(a)}}{\partial \theta_1 } = &amp;
\begin{cases}
1 &amp; \ \mbox{if $ \theta_1 = \gamma^{jg(a)}$} \\
0 &amp; \ \mbox{otherwise}
\end{cases} \\
\frac{ \partial h_{it,kg}}{\partial \gamma^{jg(a)} } = &amp;
\begin{cases}
h_{it,kg}\cdot (1- h_{it,kg}) \cdot u_{it}^{(a)}&amp; \ \mbox{if $j=k$} \\
-h_{it,kg}\cdot h_{it,jg}\cdot u_{it}^{(a)}&amp; \ \mbox{if $j \neq k$}
\end{cases}
\end{aligned}
\]</span></p>
<p>then for the columns in <span class="math inline">\(\boldsymbol{R}\)</span> corresponding to <span class="math inline">\(\gamma^{db(c)}\)</span>, the <span class="math inline">\(k^{th}\)</span> element is equal to <span class="math inline">\(\frac{\partial h_{it,kb}}{\partial \gamma^{db(c)}} \mu_{it-1,b}\)</span>. So, the column in <span class="math inline">\(\boldsymbol{R}\)</span> corresponding to <span class="math inline">\(\gamma^{db(c)}\)</span> would equal.</p>
<p><span class="math display">\[
-\left[
\begin{array}{c}
\frac{\partial h_{it,1b}}{\partial \gamma^{db(c)}} \mu_{it-1,b} \\
\frac{\partial h_{it,2b}}{\partial \gamma^{db(c)}} \mu_{it-1,b} \\
\dots \\
\frac{\partial h_{it,K-1b}}{\partial \gamma^{db(c)}} \mu_{it-1,b} \\
\end{array}
\right]
\]</span></p>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      let href = ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>