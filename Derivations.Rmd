---
title: "Marginalized, partial proportional odds models: Mathematical derivations for the likelihood and score"
author: "Jonathan Schildcrout"
date: 
output: 
 rmdformats::readthedown:
    thumbnails: false
    lightbox: true
    gallery: true
    highlight: tango
    use_bookdown: true
    toc_depth: 3
    fig_caption: true
    fig_width: 10
 # html_document:
  #  toc: yes
  #  toc_depth: 3
   # number_sections: true
    #toc_float: 
     #collapsed: false
    #code_folding: hide
    #theme: cosmo
---
::: {.hidden}
\newcommand{\Yit}{Y_{it}}
\newcommand{\yitONE}{Y_{it-1}}
\newcommand{\Yitk}{Y_{it,k}}
\newcommand{\YitK}{Y_{it,K}}
\newcommand{\Yitl}{Y_{it,l}}
\newcommand{\Yitd}{Y_{it,d}}
\newcommand{\Yitb}{Y_{it,b}}
\newcommand{\yitONEl}{Y_{it-1,l}}
\newcommand{\yiONEl}{Y_{i1,k}}
\newcommand{\yitl}{y_{it,l}}
\newcommand{\yitONEl}{y_{it-1,l}}
\newcommand{\Zitk}{Z_{it,k}}
\newcommand{\ZiONEk}{Z_{i1,k}}
\newcommand{\ZitkpONE}{Z_{it,k+1}}
\newcommand{\ZiONEkpONE}{Z_{i1,k+1}}

\newcommand{\Uit}{U_{it}}
\newcommand{\uit}{u_{it}}
\newcommand{\Uita}{U_{it}^{(a)}}
\newcommand{\uita}{u_{it}^{(a)}}

\newcommand{\mumit}[1]{\mu_{it,#1}^m}
\newcommand{\mumit2}{\mu_{it,2}^m}
\newcommand{\mumitKONE}{\mu_{it,K-1}^m}
\newcommand{\mumitk}{\mu_{it,k}^m}
\newcommand{\mumiONEk}{\mu_{i1,k}^m}
\newcommand{\mumit1g}{\mu_{it-1,g}^m}

\newcommand{\mucitk}{\mu_{it,k}^c}
\newcommand{\mucitK}{\mu_{it,K}^c}
\newcommand{\mucitd}{\mu_{it,d}^c}
\newcommand{\mucitb}{\mu_{it,b}^c}

\newcommand{\pimitk}{\pi_{it,k}^m}
\newcommand{\pimitKONE}{\pi_{it,k-1}^m}

\newcommand{\pimitkpONE}{\pi_{it,k+1}^m}
\newcommand{\pimiONEk}{\pi_{i1,k}^m}
\newcommand{\pimiONEkpONE}{\pi_{i1,k+1}^m}

\newcommand{\pimit1k}{\pi_{it-1,k}^m}
\newcommand{\hitkl}{h_{it,kl}}
\newcommand{\hitkg}{h_{it,kg}}
\newcommand{\hitjg}{h_{it,jg}}
\newcommand{\hitkK}{h_{it,kK}}
\newcommand{\hitKg}{h_{it,Kg}}
\newcommand{\hit}[2]{h_{it,#1 #2}}

\newcommand{\etaitk}{\eta_{it,k}}
\newcommand{\etaiONEk}{\eta_{i1,k}}
\newcommand{\etaiONEkpONE}{\eta_{i1,k+1}}

\newcommand{\Deltait}{\mathbf{\Delta}_{it}}
\newcommand{\Deltaitk}{\Delta_{it,k}}
\newcommand{\Deltaitl}{\Delta_{it,l}}
\newcommand{\Deltaitj}{\Delta_{it,j}}
\newcommand{\DeltaitKONE}{\Delta_{it,K-1}}
\newcommand{\Deltait1}{\Delta_{it,1}}
\newcommand{\Deltait2}{\Delta_{it,2}}

\newcommand{\Deltait1g}{\Delta_{it-1,g}}
\newcommand{\Deltait1g}{\Delta_{it-1,g}}
\newcommand{\Deltait1K1}{\Delta_{it-1,K-1}}

\newcommand{\gammakl}{\gamma^{kl}}
\newcommand{\gammajg}{\gamma^{jg}}
\newcommand{\gammalg}{\gamma^{lg}}
\newcommand{\gammakK}{\gamma^{kK}}
\newcommand{\gammakg}{\gamma^{kg}}
\newcommand{\gammadb}{\gamma^{db}}
\newcommand{\gamma1g}{\gamma^{1g}}
\newcommand{\gammaKONEg}{\gamma^{(K-1)g}}
\newcommand{\gammaKONEK1}{\gamma^{(K-1)(K-1)}}
\newcommand{\gamma11}{\gamma^{11}}

\newcommand{\gammakla}{\gamma^{kl(a)}}
\newcommand{\gammajga}{\gamma^{jg(a)}}
\newcommand{\gammalga}{\gamma^{lg(a)}}
\newcommand{\gammakKa}{\gamma^{kK(a)}}
\newcommand{\gammakga}{\gamma^{kg(a)}}
\newcommand{\gammadba}{\gamma^{db(a)}}
\newcommand{\gamma1ga}{\gamma^{1g(a)}}
\newcommand{\gammaKONEga}{\gamma^{(K-1)g(a)}}
\newcommand{\gammaKONEK1a}{\gamma^{(K-1)(K-1)(a)}}
\newcommand{\gamma11a}{\gamma^{11(a)}}
\newcommand{\gamma1g1}{\gamma^{1g(1)}}
\newcommand{\gammaKONEg1}{\gamma^{(K-1)g(1)}}
\newcommand{\gamma1gA}{\gamma^{1g(A)}}
\newcommand{\gammaKONEgA}{\gamma^{(K-1)g(A)}}


\newcommand{\phiitk}{\phi_{it,k}}
\newcommand{\phiitkpONE}{\phi_{it,k+1}}
\newcommand{\phiiONEk}{\phi_{i1,k}}
\newcommand{\phiiONEkpONE}{\phi_{i1,k+1}}


\newcommand{\bm}[1]{\boldsymbol{#1}}

\newcommand{\bA}{\mbox{\bm $A$}}
\newcommand{\bB}{\mbox{\bm $B$}}
\newcommand{\bC}{\mbox{\bm $C$}}
\newcommand{\bD}{\mbox{\bm $D$}}
\newcommand{\bG}{\mbox{\bm $G$}}
\newcommand{\bI}{\mbox{\bm $I$}}
\newcommand{\bJ}{\mbox{\bm $J$}}
\newcommand{\bL}{\mbox{\bm $L$}}
\newcommand{\bM}{\mbox{\bm $M$}}
\newcommand{\bQ}{\mbox{\bm $Q$}}
\newcommand{\bR}{\mbox{\bm $R$}}
\newcommand{\bS}{\mbox{\bm $S$}}
\newcommand{\bT}{\mbox{\bm $T$}}
\newcommand{\bV}{\mbox{\bm $V$}}
\newcommand{\bW}{\mbox{\bm $W$}}
\newcommand{\bX}{\mbox{\bm $X$}}
\newcommand{\bY}{\mbox{\bm $Y$}}
\newcommand{\bZ}{\mbox{\bm $Z$}}
\newcommand{\bN}{\mbox{\bm $N$}}
\newcommand{\bdN}{\mbox{\bm $dN$}}
\newcommand{\bb}{\mbox{\bm $b$}}
\newcommand{\bh}{\mbox{\bm $h$}}
\newcommand{\bj}{\mbox{\bm $j$}}
\newcommand{\bl}{\mbox{\bm $l$}}
\newcommand{\bmm}{\mbox{\bm $m$}}
\newcommand{\bn}{\mbox{\bm $n$}}
\newcommand{\bq}{\mbox{\bm $q$}}
\newcommand{\bs}{\mbox{\bm $s$}}
\newcommand{\bt}{\mbox{\bm $t$}}
\newcommand{\bu}{\mbox{\bm $u$}}
\newcommand{\bx}{\mbox{\bm $x$}}
\newcommand{\by}{\mbox{\bm $y$}}
\newcommand{\bz}{\mbox{\bm $z$}}
\newcommand{\bd}{\mbox{\bm $d$}}
\newcommand{\boldone}{\mbox{\bm $1$}}
\newcommand{\mbf}{\mathbf}

\newcommand{\balpha}{\bm{\alpha}}
\newcommand{\bbeta}{\bm{\beta}}
\newcommand{\bgamma}{\bm{\gamma}}
\newcommand{\bomega}{\mbox{\bm $\omega$}}
\newcommand{\bepsilon}{\mbox{\bm $\epsilon$}}
\newcommand{\bmu}{\mbox{\bm $\mu$}}
\newcommand{\btheta}{\mbox{\bm $\theta$}}
\newcommand{\boeta}{\mbox{\bm $\eta$}}
\newcommand{\bSigma}{\mbox{\bm $\Sigma$}}
\newcommand{\bphi}{\mbox{\bm $\phi$}}
\newcommand{\bGamma}{\mbox{\bm $\Gamma$}}
:::


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message = FALSE, cache=FALSE, warning = FALSE, echo=FALSE}
# load packages
# load packages
library(Rcpp)
library(knitr)      
library(kableExtra)

## Fitting functions
#source("LikelihoodFunctions.R")
#source("FunctionsRcpp.R")
#source("omtm.summary.R")
```

# Notation
$\Yit$: ordinal response for subject $i \in \{1, \dots N\}$ on day $t \in \{1, \dots T_i\}$ with values $k \in \{1, \dots, K \}$ 

$\Yitk=I(\Yit=k)$

$\Zitk=\sum_{l=1}^k \Yitk = I(\Yit \leq k)$

$\mathbf{X}_i$ is the design matrix for univariate / cross-sectional model and $\mathbf{X}_{it}$ is the design vector on day $t$.

$\mathbf{U}_i$ is the design matrix for the response dependence model and $\mathbf{U}_{it}$ is the design vector on day $t$.  In many cases, covariates will not modify the response dependence model and $\uit$ will just be a 1.

Noting that conditioning on $\mathbf{X}_i$ is implicit, then

* $\pimitk = pr(\Yit \leq k)$ is the marginal cumulative probability that captures how $Y$ depends on $X$

* $\mumitk = pr(\Yit=k) = \pimitk - \pimitKONE$ is the marginal probability of being in state $k$.

* $\mucitk = pr(\Yit=k \mid \yitONE)$ is the conditional (on the lagged response) probability of being in state $k$.  It captures how $Y$s depend on each other.  Here we are assuming a first-order Markov model.

* $\hitkl = pr(\Yit=k \mid \yitONE=l)$ where conditioning on $\uit$ is implicit


# The Marginalized Model

The marginalized transition model for ordinal response data is given by the following two models:

$$
\begin{aligned}
logit(\pimitk) &= \etaitk = \alpha_k + \mathbf{X}_{it} \mathbf{\beta} \\ 
log \left( \frac{\mucitk}{\mucitK} \right) &= \Deltaitk(\mathbf{X}_i) + \sum_{l=1}^{K-1} \uit \gammakl \yitONEl
\end{aligned}
$$

where the former is a cumulative probability, proportional odds model, and the latter is a log relative risk model for nominal data.  We do not use the ordering for the response depdendence model (yet).  If we have $\etaitk$ and $\gammakl \ \forall \ k,l$ then we can solve for the implicit function $\Deltaitk$ that constrains the marginal model and conditional model to be coherent.  Once we have $\Deltaitk$, then we can capture $\mucitk$ for all $k$, and as we will show, we can then identify the likelihood contribution for subject $i$ at time $t$.  The likelihood contribution is derived from $\mucitk$.  The $\Deltaitk$ can be thought of as an offset or an intercept for the conditional mean model.  Note $\gammakl$ may be a scalar or a vector depending on $\uit$. 

## Calculate $\Deltaitk$

Since the marginal mean model must equal the marginalized, conditional mean model (where we marginalize over the distribution of $\yitONE$), then

$$ \mumitk = \sum_{g=1}^{K} \hitkg \cdot \mumit1g = \sum_{g=1}^{K} \frac{exp(\Deltaitk + \uit \gamma^{kg})}{1+\sum_{l=1}^{K-1}exp(\Deltaitl + \uit \gamma^{lg})} \cdot \mumit1g$$

We need to solve this equation for $\Deltaitk \ \forall \ i,t,k$, and to do this we implement a Newton Raphson algorithm.  Let

$$f(\mathbf{\Delta}_{it})= \{f_k(\mathbf{\Delta}_{it}) \}_{k=1}^{K}=  \left\{\sum_{g=1}^{K} \hitkg \cdot \mumit1g - \mumitk \right\}_{k=1}^{K}$$
be a vector of length $K$ and for all $k=\{1, \dots K \}$, solve $f_k(\mathbf{\Delta}_{it})=0$.  If we look closely $\hitkg$, we see that it depends on all $\{\Delta_{it,1}, \dots \Delta_{it,K-1}\}$, so the gradient of $f(\mathbf{\Delta}_{it})$ is a non-diagonal matrix.  At iteration $m$ of the NR algorithm, we have 

$$\mathbf{\Delta}_{it}^{(m)} = \mathbf{\Delta}_{it}^{(m-1)} - \left( \frac{ \partial f(\mathbf{\Delta}_{it}^{(m-1)})}{\partial \mathbf{\Delta}_{it}^{(m-1)}}   \right)^{-1} f(\mathbf{\Delta}_{it}^{(m-1)})$$
where
$$
\frac{ \partial f_j(\mathbf{\Delta}_{it})}{\partial \Deltaitk} =
\begin{cases}
\sum_{g=1}^{K} \hitkg \cdot (1- \hitkg) \cdot \mumit1g,& \ \mbox{if $j=k$} \\
\sum_{g=1}^{K} -\hitkg \cdot \hitjg \cdot \mumit1g,& \ \mbox{if $j \neq k$} \\
\end{cases}
$$

# Notes about conditional mean model

* $\hitkg = pr(\Yitk \mid \yitONE=g) = \frac{exp(\Deltaitk + \uit \gammakg)}{1+\sum_{l=1}^{K-1} exp(\Deltaitl + \uit \gammalg)}$
* $\hitkK = pr(\Yitk \mid \yitONE=K)= \frac{exp(\Deltaitk)}{1+\sum_{l=1}^{K-1} exp(\Deltaitl)}$ because state $K$ is the reference state for lagged values (i.e. $\gammakK=0$). So $\gamma$ does not appear in the $K^{th}$ column of the $\mathbf{h}_{it}$ matrix.
* $\hitKg =  1-\sum_{k=1}^{K-1} pr(\Yitk \mid \yitONE=g) = 1-\sum_{k=1}^{K-1} \hitkg = \frac{1}{1+\sum_{l=1}^{K-1} exp(\Deltaitl+ \uit \gammalg)}$ because state $K$ is the reference state for the outcome
* In the $\mathbf{h}_{it}$ matrix ($K \times K$), rows correspond to $\Yit$ and columns correspond to $\yitONE$, so that the $(k,g)^{th}$ element is $\hitkg = pr(\Yitk \mid \yitONE=g)$.
* $\Deltaitk$ is the fixed intercept for a row in $\mathbf{h}_{it}$

$$
\mathbf{h}_{it} = \left\{
        \begin{array}{cccc}
        \hit{1}{1} & \hit{1}{2} & \dots & \hit{1}{K} \\
        \hit{2}{1} & \hit{2}{2} & \dots & \hit{2}{K} \\
        \dots      & \dots      & \dots & \dots \\
        \hit{K-1}{1} & \hit{K-1}{2} & \dots & \hit{K-1}{K} \\
        1-\sum_{l=1}^{K-1} \hit{l}{1} & 1-\sum_{l=1}^{K-1} \hit{l}{1} & \dots & 1-\sum_{l=1}^{K-1} \hit{l}{1} \\
        \end{array}
        \right\}
$$

* $log \left( \frac{ pr(\Yit=k \mid \yitONE=g)}{pr(\Yit=K \mid \yitONE=g)} \right) = \Deltaitk + \uit \gammakg$ and $log \left( \frac{ pr(\Yit=k \mid \yitONE=K)}{pr(\Yit=K \mid \yitONE=K)} \right) = \Deltaitk$, so


$$\gammakg=  log \left( \frac{ pr(\Yit=k \mid \yitONE=g)/pr(\Yit=K \mid \yitONE=g)  }{ pr(\Yit=k \mid \yitONE=K)/pr(\Yit=K \mid \yitONE=K) } \right)  = log \left( \frac{ \hit{k}{g} / \hit{K}{g}  }{  \hit{k}{K} / \hit{K}{K} } \right)$$

is not that easy (for me) to interpret by itself bc we need to know values of all elements of $\mathbf{\Delta}_{it}$.

* Think of $\uit \{ \gammakg \}$ as a $(K-1) \times (K-1)$ matrix of log relative risk ratios (that depend on $\uit$) for transitions from state $g$ to state $k$, where rows $k$ correspond to outcomes on day $t$ and columns $g$ correspond to outcomes on day $t-1$. 

# The Likelihood

Since this is a first-order transition model, we split the likelihood contribution for each subject into the 1) the first observation, and 2) all other observations.  Let $\mathbf{\theta}=(\mathbf{\alpha}, \mathbf{\beta}, \mathbf{\gamma})$

* $L(\mathbf{\theta}) = \prod_{i=1}^N L_i(\mathbf{\theta})$
* $l(\mathbf{\theta}) = \sum_{i=1}^N l_i(\mathbf{\theta}) = \sum_{i=1}^N log L_i(\mathbf{\theta})$

The log-likelihood contribution by subject $i$ can be written,

$$
\begin{aligned}
l_i(\mathbf{\theta}) =&  l_i^{(1)}(\mathbf{\beta},\mathbf{\alpha}) + l_i^{(2)}(\mathbf{\theta}) \\
=& \sum_{k=1}^{K} \yiONEl log (\mumiONEk) + \sum_{t=2}^{T_i} \sum_{k=1}^{K} \Yitk log (\mucitk)
\end{aligned}
$$

Relying on Lee and Daniels (2007) and McCullagh (1980), and for computational efficiency, we can write $l_i^{(1)}(\mathbf{\beta},\mathbf{\alpha})$ as follows

$$
\begin{aligned}
l_i^{(1)}(\mathbf{\beta},\mathbf{\alpha})= \sum_{k=1}^{K-1} [\ZiONEk \phiiONEk - \ZiONEkpONE g(\phiiONEk)] 
\end{aligned}
$$

where

* $\phiitk = log \left( \frac{\pimitk}{\pimitkpONE-\pimitk} \right)$
* $g(\phiitk) = log(1+exp(\phiitk)) = log \left( \frac{\pimitkpONE}{\pimitkpONE-\pimitk} \right)$


We can write $l_i^{(2)}(\mathbf{\theta})$ in exponential family form as follows

$$
\begin{aligned}
l_i^{(2)}(\mathbf{\theta})= \sum_{t=2}^{T_i} \left[ \sum_{k=1}^{K-1} \Yitk (\Deltaitk + \sum_{l=1}^{K-1} \uit \gammakl \yitONEl) - log(1+\sum_{k=1}^{K-1} exp(\Deltaitk + \sum_{l=1}^{K-1} \uit \gammakl \yitONEl)) \right] 
\end{aligned}
$$

# Gradients

$$
\begin{aligned}
\frac{\partial l_i^{(1)}(\mathbf{\beta},\mathbf{\alpha})}{\partial \theta_1}= \sum_{k=1}^{K-1} \left[ \ZiONEk \frac{\partial \phiiONEk}{\partial \theta_1} - \ZiONEkpONE \frac{\partial g(\phiiONEk)}{\partial \theta_1} \right]  
\end{aligned}
$$

where we can show that (see handwritten notes)

* $\frac{\partial \phiiONEk}{\partial \theta_1}=exp(g(\phiiONEk)) \cdot \left[(1-\pimiONEk) \frac{\partial \etaiONEk}{\partial \theta_1} - (1-\pimiONEkpONE) \frac{\partial \etaiONEkpONE}{\partial \theta_1}   \right]$
* $\frac{\partial g(\phiiONEk)}{\partial \theta_1}=exp(\phiiONEk) \cdot \left[(1-\pimiONEk) \frac{\partial \etaiONEk}{\partial \theta_1} - (1-\pimiONEkpONE) \frac{\partial \etaiONEkpONE}{\partial \theta_1}   \right] = \frac{exp(\phiiONEk)}{exp(g(\phiiONEk))} \cdot \frac{\partial \phiiONEk}{\partial \theta_1} = \frac{\pimiONEk}{\pimiONEkpONE} \cdot \frac{\partial \phiiONEk}{\partial \theta_1}$

so that 

$$
\begin{aligned}
\frac{\partial l_i^{(1)}(\mathbf{\beta},\mathbf{\alpha})}{\partial \theta_1}= \sum_{k=1}^{K-1} \left[ \ZiONEk  - \ZiONEkpONE \frac{\pimiONEk}{\pimiONEkpONE} \right]  \frac{\partial \phiiONEk}{\partial \theta_1}
\end{aligned}
$$

where 

* $\frac{\pimiONEk}{\pimiONEkpONE}=\frac{exp(\phiiONEk)}{1+exp(\phiiONEk)}$
* $\frac{\partial \phiiONEk}{\partial \theta_1} = \frac{\partial \phiiONEk}{\partial \pimiONEk} \cdot \frac{\partial \pimiONEk}{\partial \etaiONEk} \cdot \frac{\partial \etaiONEk}{\partial \theta_1} + \frac{\partial \phiiONEk}{\partial \pimiONEkpONE} \cdot \frac{\partial \pimiONEkpONE}{\partial \etaiONEkpONE} \cdot \frac{\partial \etaiONEkpONE}{\partial \theta_1} =  \frac{\partial \phiiONEk}{\partial \etaiONEk} \cdot \frac{\partial \etaiONEk}{\partial \theta_1} + \frac{\partial \phiiONEk}{\partial \etaiONEkpONE} \cdot \frac{\partial \etaiONEkpONE}{\partial \theta_1}$.  This is from LD2007 and I checked that this is equal to what I got.


<!-- $$ -->
<!-- \begin{aligned} -->
<!-- # \frac{\partial l_i^{(2)}(\mathbf{\theta})}{\partial \theta_1}= & \sum_{t=2}^{T_i} \left[ \sum_{k=1}^{K-1} \Yitk \left(\frac{\partial \Deltaitk}{\partial \theta_1} + \sum_{l=1}^{K-1} \frac{\partial \gammakl}{\partial \theta_1} \yitONEl \right)  - \frac{exp(\Deltaitk + \sum_{l=1}^{K-1} \gammakl \yitONEl)}{(1+\sum_{k=1}^{K-1} exp(\Deltaitk + \sum_{l=1}^{K-1} \gammakl \yitONEl))} \left(\frac{\partial \Deltaitk}{\partial \theta_1} + \sum_{l=1}^{K-1} \frac{\partial \gammakl}{\partial \theta_1} \yitONEl \right) \right] \\  -->
<!-- \end{aligned} -->
<!-- $$ -->

Next, 
$$
\begin{aligned}
\frac{\partial l_i^{(2)}(\mathbf{\theta})}{\partial \theta_1} =& \sum_{t=2}^{T_i} \left[ \sum_{k=1}^{K-1} (\Yitk - \mucitk) \left(\frac{\partial \Deltaitk}{\partial \theta_1} + \sum_{l=1}^{K-1} \uit \frac{\partial \gammakl}{\partial \theta_1} \yitONEl \right)  \right]
\end{aligned}
$$


so that

$$
\begin{aligned}
\frac{\partial l_i^{(2)}(\mathbf{\theta})}{\partial \alpha_k}= & \sum_{t=2}^{T_i} \left[ \sum_{k=1}^{K-1} (\Yitk - \mucitk) \left(\frac{\partial \Deltaitk}{\partial \alpha_k}  \right)  \right] \\
\frac{\partial l_i^{(2)}(\mathbf{\theta})}{\partial \beta_b}= & \sum_{t=2}^{T_i} \left[ \sum_{k=1}^{K-1} (\Yitk - \mucitk) \left(\frac{\partial \Deltaitk}{\partial \beta_j}  \right)  \right] \\
\frac{\partial l_i^{(2)}(\mathbf{\theta})}{\partial \gammadba}= & \sum_{t=2}^{T_i} \left[ \sum_{k=1}^{K-1} (\Yitk - \mucitk) \left(\frac{\partial \Deltaitk}{\partial \gammadba} \right) \right] + \sum_{t=2}^{T_i}  (\Yitd-\mucitd) \uita \Yitb 
\end{aligned}
$$

because $\frac{\partial \gammakl}{\partial \gammadba}=1$ if $kl=db(a)$ and is $0$ otherwise.  $\uit$ is potentially a row vector of length $A$, $\uita$ is the $a^{th}$ element and $\gammadba$ is the $a^{th}$ element of $\gammadb$, so that 

$$
\begin{aligned}
\frac{\partial}{\partial \gammadba} \left[ \uit \gammakl \right] = &
\begin{cases}
\uita & kl=db \\
0 & \mbox{otherwise}
\end{cases} 
\end{aligned}
$$

## Calculation of $\frac{\partial \bm{\Delta}_{it}}{\partial \bm{\theta}}$

$\frac{\partial \bm{\Delta}_{it}}{\partial \bm{\theta}}$ is a $(K-1) \times npar$ matrix, with rows denoted by $k$ and columns corresponding to individual parameters $\theta_1 \in \bm{\theta}$.  To calculate this matrix we have to solve a system of equations $\bm{P}\bm{Q}=\bm{R}$ where $\bm{Q}=\frac{\partial \bm{\Delta}_{it}}{\partial \bm{\theta}}$. 

Now, consider a single parameter, $\theta_1$ and outcome level, $k$ which involves a single element in the $\frac{\partial \bm{\Delta}_{it}}{\partial \bm{\theta}}$ matrix.  We have
$$ 
\begin{aligned}
\mumitk = & \sum_{g=1}^{K} \hitkg \mumit1g  \\
\frac{\partial \mumitk}{\partial \theta_1} = & \sum_{g=1}^{K} \left[ \frac{\partial \hitkg}{\partial \theta_1}\mumit1g   + \hitkg\frac{\partial \mumit1g}{\partial \theta_1} \right] \\
\frac{\partial \hitkg}{\partial \theta_1}\mumit1g = & \frac{\partial \mumitk}{\partial \theta_1} - \hitkg\frac{\partial \mumit1g}{\partial \theta_1} \\
\end{aligned}
$$
Since $\hitkg=h(\Deltait1, \dots \DeltaitKONE, \gamma1g1, \dots \gammaKONEg1, \dots \gamma1gA, \dots \gammaKONEgA)$

$$ 
\begin{aligned}
\frac{\partial \hitkg}{\partial \theta_1} = & \sum_{j=1}^{K-1} \frac{\partial \hitkg}{\partial \Deltaitj} \frac{\partial \Deltaitj}{\partial \theta_1} + \sum_{a=1}^{A} \sum_{j=1}^{K=1}  \frac{\partial \hitkg}{\partial \gammajga} \frac{\partial \gammajga}{\partial \theta_1} 
\end{aligned}
$$

$$ 
\begin{aligned}
 \underbrace{\sum_{g=1}^{K}   \sum_{j=1}^{K-1}  \frac{\partial \hitkg}{\partial \Deltaitj} \frac{\partial \Deltaitj}{\partial \theta_1}\mumit1g}_{(i)} = - \underbrace{ \sum_{g=1}^{K} \sum_{a=1}^{A} \sum_{j=1}^{K-1} \frac{\partial \hitkg}{\partial \gammajg} \frac{\partial \gammajg}{\partial \theta_1} \mumit1g}_{(ii)}  + \underbrace{\frac{\partial \mumitk}{\partial \theta_1}}_{(iii)} - \sum_{g=1}^{K} \underbrace{\hitkg\frac{\partial \mumit1g}{\partial \theta_1}}_{(iv)}
\end{aligned}
$$
We need to solve the above for all $K-1 \times npar$ outcome level by parameter combinations simultaneously.
The value $(i)$ corresponds to the multiplkation of $k^{th}$ row of the left matrix $\bm{P}$ with the column corresponding to $\theta_1$ in the right matrix $\bm{Q}$:

$$
\underbrace{
\left[
\begin{array}{cccc}
\frac{\partial h_{it,1*}}{\partial \Delta_{it,1}} \cdot \mu_{it-1,*} &  \frac{\partial h_{it,1*}}{\partial \Delta_{it,2}}\mu_{it-1,*} & \dots & \frac{\partial h_{it,1*}}{\partial \Delta_{it,K-1}}\mu_{it-1,*} \\
\frac{\partial h_{it,2*}}{\partial \Delta_{it,1}} \cdot \mu_{it-1,*} &  \frac{\partial h_{it,2*}}{\partial \Delta_{it,2}}\mu_{it-1,*} & \dots & \frac{\partial h_{it,2*}}{\partial \Delta_{it,K-1}}\mu_{it-1,*} \\
. && \dots & \\
. && \dots & \\
. && \dots & \\
\frac{\partial h_{it,K-1*}}{\partial \Delta_{it,1}} \cdot \mu_{it-1,*} &  \frac{\partial h_{it,K-1*}}{\partial \Delta_{it,2}}\mu_{it-1,*} & \dots & \frac{\partial h_{it,K-1*}}{\partial \Delta_{it,K-1}}\mu_{it-1,*} 
\end{array}
\right]
}_{\bm{P} \sim (K-1) \times (K-1)}
\underbrace{\left[
\begin{array}{cccc}
\frac{\partial  \Delta_{it,1}}{\partial \theta_1 } & \frac{\partial  \Delta_{it,1}}{\partial \theta_2 } \dots \frac{\partial  \Delta_{it,1}}{\partial \theta_{npar} } \\
\frac{\partial  \Delta_{it,2}}{\partial \theta_1 }  & \frac{\partial  \Delta_{it,2}}{\partial \theta_2 } \dots \frac{\partial  \Delta_{it,2}}{\partial \theta_{npar} } \\
. \\
. \\
.\\
\frac{\partial  \Delta_{it,K-1}}{\partial \theta_1 }  & \frac{\partial  \Delta_{it,K-1}}{\partial \theta_2 } \dots \frac{\partial  \Delta_{it,K-1}}{\partial \theta_{npar} }    
\end{array}
\right]}_{\bm{Q} \sim (K-1) \times npar}
$$
where

$$
\begin{aligned}
\frac{ \partial \hitkg}{\partial \Deltaitj } = &
\begin{cases}
\hitkg \cdot (1- \hitkg),& \ \mbox{if $j=k$} \\
-\hitkg \cdot \hitjg ,& \ \mbox{if $j \neq k$}.
\end{cases} 
\end{aligned}
$$

Assuming that $K=4$, then $\bm{P}$ is equal to 

$$
\left[
\begin{array}{ccc}
[\mathbf{h}_{it,1*} \circ (\mathbf{1}-\mathbf{h}_{it,1*})] \mathbf{\mu}_{it-1,*}^m & 
[\mathbf{h}_{it,1*} \circ (-\mathbf{h}_{it,2*})] \mathbf{\mu}_{it-1,*}^m &
[\mathbf{h}_{it,1*} \circ (-\mathbf{h}_{it,3*})] \mathbf{\mu}_{it-1,*}^m \\
[\mathbf{h}_{it,2*} \circ (-\mathbf{h}_{it,1*})] \mathbf{\mu}_{it-1,*}^m &
[\mathbf{h}_{it,2*} \circ (\mathbf{1}-\mathbf{h}_{it,2*})] \mathbf{\mu}_{it-1,*}^m &
[\mathbf{h}_{it,2*} \circ (-\mathbf{h}_{it,3*})] \mathbf{\mu}_{it-1,*}^m \\
[\mathbf{h}_{it,3*} \circ (-\mathbf{h}_{it,1*})] \mathbf{\mu}_{it-1,*}^m &
[\mathbf{h}_{it,3*} \circ (-\mathbf{h}_{it,2*})] \mathbf{\mu}_{it-1,*}^m &
[\mathbf{h}_{it,3*} \circ (\mathbf{1}-\mathbf{h}_{it,3*})] \mathbf{\mu}_{it-1,*}^m \\
\end{array}
\right]
$$
where we use $\circ$ to denote Hadamard (elementwise) multiplication, $\bm{h}_{it,k*}$ is a row vector, and $\bm{\mu}_{it-1,*}^m$ is a column vector.

For $\theta_1 \in (\mathbf{\alpha}, \mathbf{\beta})$, where  $ii=0$, a column of $\bm{R}$ corresponding to $\theta_1$ is equal to: 
$$
\begin{aligned}
\left[
\begin{array}{c}
\frac{\partial \mumit{1}}{\partial \theta_1} - \sum_{g=1}^{K} \hit{1}{g} \frac{\partial \mumit1g}{\partial \theta_1} & \\ 
\frac{\partial \mumit{2}}{\partial \theta_1} - \sum_{g=1}^{K} \hit{2}{g} \frac{\partial \mumit1g}{\partial \theta_1} & \\
\dots \\
\frac{\partial \mumit{K-1}}{\partial \theta_1} - \sum_{g=1}^{K} \hit{K-1}{g} \frac{\partial \mumit1g}{\partial \theta_1} 
\end{array}
\right]
=&
\left[
\begin{array}{c}
\frac{\partial \mumit{1}}{\partial \theta_1} - \hit{1}{*} \frac{\partial \mu_{it-1,*}^m}{\partial \theta_1} & \\ 
\frac{\partial \mumit{2}}{\partial \theta_1} -  \hit{2}{*} \frac{\partial \mu_{it-1,*}^m}{\partial \theta_1} & \\
\dots \\
\frac{\partial \mumit{K-1}}{\partial \theta_1} - \hit{K-1}{*} \frac{\partial \mu_{it-1,*}^m}{\partial \theta_1} 
\end{array}
\right]
\end{aligned}
$$
where, again $\bm{h}_{it,k*}$ is a row vector, and $\frac{\partial \mu_{it-1,*}^m}{\partial \theta_1}$  is a column vector.  Across all of the $(\bm{\alpha},\bm{\beta})$, and assuming $K=4$ and $\bm{x}_{it}= (x_{it,1}, x_{it,2})$, the $\frac{\partial \bm{\mu}_{it}^m}{\partial \bm{\theta}}$ is equal to 

$$
\left[
\begin{array}{ccccc}
\frac{\partial \pi_{it,1}}{\partial \alpha_1} - 0 & \frac{\partial \pi_{it,1}}{\partial \alpha_2} - 0 & \frac{\partial \pi_{it,1}}{\partial \alpha_3} - 0 & \frac{\partial \pi_{it,1}}{\partial \beta_1} - 0 & \frac{\partial \pi_{it,1}}{\partial \beta_2} - 0 \\   
\frac{\partial \pi_{it,2}}{\partial \alpha_1} - \frac{\partial \pi_{it,1}}{\partial \alpha_1} & 
\frac{\partial \pi_{it,2}}{\partial \alpha_2} - \frac{\partial \pi_{it,1}}{\partial \alpha_2} & 
\frac{\partial \pi_{it,2}}{\partial \alpha_3} - \frac{\partial \pi_{it,1}}{\partial \alpha_3}  & 
\frac{\partial \pi_{it,2}}{\partial \beta_1} -  \frac{\partial \pi_{it,1}}{\partial \beta_1} &   
\frac{\partial \pi_{it,2}}{\partial \beta_2} -  \frac{\partial \pi_{it,1}}{\partial \beta_2} \\
\frac{\partial \pi_{it,3}}{\partial \alpha_1} - \frac{\partial \pi_{it,2}}{\partial \alpha_1} & 
\frac{\partial \pi_{it,3}}{\partial \alpha_2} - \frac{\partial \pi_{it,2}}{\partial \alpha_2} & 
\frac{\partial \pi_{it,3}}{\partial \alpha_3} - \frac{\partial \pi_{it,2}}{\partial \alpha_3}  & 
\frac{\partial \pi_{it,3}}{\partial \beta_1} -  \frac{\partial \pi_{it,2}}{\partial \beta_1} &   
\frac{\partial \pi_{it,3}}{\partial \beta_2} -  \frac{\partial \pi_{it,2}}{\partial \beta_2} \\
0 - \frac{\partial \pi_{it,3}}{\partial \alpha_1} & 
0 - \frac{\partial \pi_{it,3}}{\partial \alpha_2} &
0 - \frac{\partial \pi_{it,3}}{\partial \alpha_3} &
0 - \frac{\partial \pi_{it,3}}{\partial \beta_1}  &   
0 - \frac{\partial \pi_{it,3}}{\partial \beta_2} 
\end{array}
\right]
$$
which for programming purposes we get from 

$$
\left[
\begin{array}{c}
\frac{\partial \pi_{it,1}}{\partial \eta_1} \\
\frac{\partial \pi_{it,2}}{\partial \eta_2} \\
\frac{\partial \pi_{it,3}}{\partial \eta_3} \\
0
\end{array}
\right]
\left[
\begin{array}{ccccc}
1 & 0 & 0 & x_{it,1} & x_{it,2} \\
0 & 1 & 0 & x_{it,1} & x_{it,2} \\
0 & 0 & 1 & x_{it,1} & x_{it,2} \\
0 & 0 & 0 & 0 & 0
\end{array}
\right]
- 
\left[
\begin{array}{c}
0 \\
\frac{\partial \pi_{it,1}}{\partial \eta_1} \\
\frac{\partial \pi_{it,2}}{\partial \eta_2} \\
\frac{\partial \pi_{it,3}}{\partial \eta_3} \\
\end{array}
\right]
\left[
\begin{array}{ccccc}
0 & 0 & 0 & 0 & 0 \\
1 & 0 & 0 & x_{it,1} & x_{it,2} \\
0 & 1 & 0 & x_{it,1} & x_{it,2} \\
0 & 0 & 1 & x_{it,1} & x_{it,2} \\
\end{array}
\right]
$$
and the column vectors elements multiply the entire row of the matrix to the right.  Note also that this is the precise form for $\frac{\partial \bm{\mu}_{it-1}^m}{\partial \bm{\theta}}$ which is pre-multiplied by the row vector $\bm{h}_{it,k*}$
<!-- For each element of $\theta_1$, there are $K-1$ unknown values $\left(\frac{\partial \Deltait1}{\partial \theta_1}, \dots \frac{\partial \DeltaitKONE}{\partial \theta_1}  \right)$ and there are $K-1$ equations.  So, we need to solve the system of equations.  With $npar$ parameters, we have $npar \dot (K-1)$ values of $\frac{\partial \Deltait}{\partial \bm{\theta}}$ for all $i$ and all $t$.  \\  -->

<!-- Across all parameters, we solve $\mathbf{P}\mathbf{Q}=\mathbf{R}$ for $\mathbf{Q}$ where $\mathbf{P} \sim (K-1)\times(K-1)$, $\mathbf{Q} \sim (K-1) \times npar$, and $\mathbf{R}\sim (K-1) \times npar = -ii+iii-iv$.  -->

__For $\theta_1 \in (\gamma^{11(1)}, \dots \gamma^{(K-1)(K-1)(A)})$, $(iii)=(iv)=0$__ so that an element in $\bm{R}$ corresponding to a $\gamma$ parameter is equal to

$$
\begin{aligned}
 - \sum_{g=1}^{K} \sum_{j=1}^{K=1} \sum_{a=1}^A \frac{\partial \hitkg}{\partial \gamma^{jg(a)}} \frac{\partial \gamma^{jg(a)}}{\partial \theta_1} \mumit1g
\end{aligned}
$$

noting that since
$$
\begin{aligned}
\frac{ \partial \gamma^{jg(a)}}{\partial \theta_1 } = &
\begin{cases}
1 & \ \mbox{if $ \theta_1 = \gamma^{jg(a)}$} \\
0 & \ \mbox{otherwise}
\end{cases} \\
\frac{ \partial \hitkg}{\partial \gamma^{jg(a)} } = &
\begin{cases}
\hitkg \cdot (1- \hitkg) \cdot \uita & \ \mbox{if $j=k$} \\
-\hitkg \cdot \hitjg  \cdot \uita   & \ \mbox{if $j \neq k$}
\end{cases} 
\end{aligned}
$$

then for the columns in $\bm{R}$ corresponding to $\gamma^{db(c)}$, the $k^{th}$ element is equal to $\frac{\partial h_{it,kb}}{\partial \gamma^{db(c)}} \mu_{it-1,b}$.  So, the column in $\bm{R}$ corresponding to $\gamma^{db(c)}$ would equal.

$$
-\left[
\begin{array}{c}
\frac{\partial h_{it,1b}}{\partial \gamma^{db(c)}} \mu_{it-1,b} \\ 
\frac{\partial h_{it,2b}}{\partial \gamma^{db(c)}} \mu_{it-1,b} \\ 
\dots \\
\frac{\partial h_{it,K-1b}}{\partial \gamma^{db(c)}} \mu_{it-1,b} \\ 
\end{array}
\right]
$$
